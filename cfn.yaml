AWSTemplateFormatVersion: 2010-09-09
Description: Deploy VQT application, and EMR / Glue / Athena backend 
Parameters:
  # pDomainName: # Required
  #   Description: 'Existing DNS Domain name of the CDN, e.g. assets.example.com'
  #   Type: String
  # pCFCertificate: # Required
  #   Description: 'Existing ACM Certificate ARN for CloudFront. Must be created in the us-east-1 region!'
  #   Type: String
  pForceHttps: # Optional
    Type: String
    Description: Force HTTPS by redirecting HTTP requests
    Default: "true"
    AllowedValues:
      - "true"
      - "false"
  pEC2KeyName: # Required
    Description: Existing SSH key pair to use for EMR node login
    Type: AWS::EC2::KeyPair::KeyName
  pSubnet: # Required
    Description: Existing Subnet for EMR nodes
    Type: AWS::EC2::Subnet::Id
  pNumberofNodes: # Optional
    Description: Number of EMR nodes to provision (1-20)
    Type: Number
    Default: '1'
    MinValue: '1'
    MaxValue: '20'
  pCoreInstanceType: # Optional
    Description: EMR node ec2 instance type.
    Type: String
    Default: m4.large
    AllowedValues:
      - m2.xlarge
      - m2.2xlarge
      - m2.4xlarge
      - r3.xlarge
      - r3.2xlarge
      - r3.4xlarge
      - r3.8xlarge
      - cr1.8xlarge
      - m4.large
      - m4.xlarge
      - m4.2xlarge
      - m4.4xlarge
      - m4.10xlarge
      - m4.16xlarge
      - r4.large
      - r4.xlarge
      - r4.2xlarge
      - r4.4xlarge
      - r4.8xlarge
      - r4.16xlarge
  pMasterInstanceType: # Optional
    Description: EMR node ec2 instance type.
    Type: String
    Default: m4.large
    AllowedValues:
      - m2.xlarge
      - m2.2xlarge
      - m2.4xlarge
      - r3.xlarge
      - r3.2xlarge
      - r3.4xlarge
      - r3.8xlarge
      - cr1.8xlarge
      - m4.large
      - m4.xlarge
      - m4.2xlarge
      - m4.4xlarge
      - m4.10xlarge
      - m4.16xlarge
      - r4.large
      - r4.xlarge
      - r4.2xlarge
      - r4.4xlarge
      - r4.8xlarge
      - r4.16xlarge
  pEMRRelease: # Optional
    Description: EMR version release
    Type: String
    Default: emr-5.8.0
    AllowedValues:
      - emr-5.8.0
      - emr-5.7.0
  pDriverMemory: # Optional
      Description: EMR Spark Driver Memroy in GB - example 4G
      Type: String
      Default: 1G
  pExecutorMemory: # Optional
      Description: EMR Spark Executor Memroy in GB - example 4G
      Type: String
      Default: 1G
  pEmrCodeBucket: # Required
    Type: String
    Description: Existing S3 bucket containing Lambda and ADAM bootstrap code
    AllowedPattern: ^(?!-)[a-z0-9-]*(?<!-)$
    MinLength: 3
    MaxLength: 63
  pVCFBucket: # Required
    Type: String
    Description: New S3 bucket to load input VCF files
    AllowedPattern: ^(?!-)[a-z0-9-]*(?<!-)$
    MinLength: 3
    MaxLength: 63
  pVariantsBucket: # Required
    Type: String
    Description: New S3 bucket to write output Parquet formatted VCF files
    AllowedPattern: ^(?!-)[a-z0-9-]*(?<!-)$
    MinLength: 3
    MaxLength: 63
  pAnnotationsBucket: # Optional
    Description: Existing S3 bucket to read input Clinvar Parquet formatted VCF file
    Type: String
    Default: "vqt-annotations"
    AllowedPattern: ^(?!-)[a-z0-9-]*(?<!-)$
    MinLength: 3
    MaxLength: 63
  pSNSEmailAddress: # Optional
    Description: Email address to notify EMR cluster & Step Job status
    Type: String
    Default: example@example.com
    AllowedPattern: ^(.+)@(.+)$
  pLoggingLevel: # Optional
    Description: Logging level for Lambda Function - INFO,ERROR,WARNING
    Type: String
    Default: INFO
    AllowedValues:
      - ERROR
      - WARNING
      - INFO
  pGlueDatabase: # Optional
    AllowedPattern: ^[0-9a-z-]*$
    Type: String
    MinLength: 1
    MaxLength: 25
    Default: vqt-db
    Description: New Athena Database Name
Conditions:
  cHasForceHttps: !Equals [ !Ref pForceHttps, 'true' ]
Resources:
  IdentityPool: # Create an Identity Pool for authorited/unauthorized users
    Type: 'AWS::Cognito::IdentityPool'
    Properties:
      IdentityPoolName: vqtIdentityPool
      AllowUnauthenticatedIdentities: true
  CognitoAuthorizedRole: # Create a role for users authorized by Cognito
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument: 
        Version: "2012-10-17"
        Statement:
          - Effect: "Allow"
            Principal: 
              Federated: "cognito-identity.amazonaws.com"
            Action: 
              - "sts:AssumeRoleWithWebIdentity"
            Condition:
              StringEquals: 
                "cognito-identity.amazonaws.com:aud": !Ref IdentityPool
              "ForAnyValue:StringLike":
                "cognito-identity.amazonaws.com:amr": authenticated
      Policies:
        - PolicyName: "CognitoAuthorizedPolicy"
          PolicyDocument: 
            Version: "2012-10-17"
            Statement: 
              - Effect: "Allow"
                Action:
                  - "mobileanalytics:PutEvents"
                  - "cognito-sync:*"
                  - "cognito-identity:*"
                Resource: "*"
              - Effect: "Allow"
                Action:
                  - "lambda:InvokeFunction"
                Resource: "*"
  CognitoUnAuthorizedRole: # Create a role for users unauthorized by Cognito
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument: 
        Version: "2012-10-17"
        Statement:
          - Effect: "Allow"
            Principal: 
              Federated: "cognito-identity.amazonaws.com"
            Action: 
              - "sts:AssumeRoleWithWebIdentity"
            Condition:
              StringEquals: 
                "cognito-identity.amazonaws.com:aud": !Ref IdentityPool
              "ForAnyValue:StringLike":
                "cognito-identity.amazonaws.com:amr": unauthenticated
      Policies:
        - PolicyName: "CognitoUnauthorizedPolicy"
          PolicyDocument: 
            Version: "2012-10-17"
            Statement: 
              - Effect: "Allow"
                Action:
                  - "mobileanalytics:PutEvents"
                  - "cognito-sync:*"
                  - "cognito-identity:*"
                Resource: "*"
              - Effect: "Allow"
                Action:
                  - "lambda:InvokeFunction"
                Resource: "*"
  IdentityPoolRoleMapping: # Assign authorized/unauthorized roles to the Identity Pool
    Type: "AWS::Cognito::IdentityPoolRoleAttachment"
    Properties:
      IdentityPoolId: !Ref IdentityPool
      Roles:
        authenticated: !GetAtt CognitoAuthorizedRole.Arn
        unauthenticated: !GetAtt CognitoUnAuthorizedRole.Arn
  AthenaQueryResultsBucket: # Create an S3 bucket for athena query results
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub "aws-athena-query-results-${AWS::StackName}"
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
  AthenaLambdaRole: # Create a role for executing athena through lambda
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument: 
        Version: "2012-10-17"
        Statement:
          - Effect: "Allow"
            Principal: 
              Service: 
                - "lambda.amazonaws.com"
            Action: 
              - "sts:AssumeRole"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonAthenaFullAccess
  AthenaLambdaFunction: # Create a lambda function executable by the lambda role
    Type: "AWS::Lambda::Function"
    Properties:
      Description: Lambda function for Athena queries.
      FunctionName: !Sub "${AWS::StackName}-lambda-function"
      Handler: "index.handler"
      Role: !GetAtt AthenaLambdaRole.Arn
      Runtime: "nodejs6.10"
      Timeout: 30
      Code:
        ZipFile: !Sub >
          var AWS = require('aws-sdk');
          var athena = new AWS.Athena({
              region: 'us-east-1',
              apiVersion: '2017-05-18'
          });

          var MAX_RETRIES = 20;
          var POLLING_INTERVAL = 1000;
          var RESOLVE_STATES = ['SUCCEEDED', 'FAILED', 'CANCELLED'];

          function startQueryExecution(params) {
            return new Promise(function(resolve, reject){
              athena.startQueryExecution(params, function(err, data) {
                if (err) {
                  reject(err);
                } else {
                  resolve(data);
                }
              });
            });
          }

          function getQueryResults(queryId) {
            var params = {
              QueryExecutionId: queryId, /* required */
              // MaxResults: 0,
              // NextToken: 'STRING_VALUE'
            };

            return new Promise(function(resolve, reject) {
              athena.getQueryResults(params, function(err, data) {
                if (err) {
                  reject(err);
                } else {
                  resolve(data);
                }
              });
            });
          }

          function pollForChanges(queryId) {
            var numTries = 0;
            var params = {
              QueryExecutionId: queryId, /* required */
              // MaxResults: 0,
              // NextToken: 'STRING_VALUE'
            };

            return new Promise(function(resolve, reject) {
              var myInterval = setInterval(function(){
                numTries ++;
                if (numTries > MAX_RETRIES) {
                  clearInterval(myInterval);
                  reject(new Error('MAX_RETRIES exceeded.'));
                  return;
                }

                athena.getQueryExecution(params, function(err, data) {
                  if (err) {
                    clearInterval(myInterval);
                    reject(err);
                  } else {
                    var state = data.QueryExecution.Status.State;
                    // This means state is a member of RESOLVE_STATES
                    if (~RESOLVE_STATES.indexOf(state)) {
                      clearInterval(myInterval);
                      if (state === 'SUCCEEDED') {
                          resolve(state);
                      } else {
                          reject(new Error('State ' + state + ' is NO GOOD.'));
                      }
                    }
                  }
                });
              }, POLLING_INTERVAL);
            });
          }

          exports.handler = function(event, context) {
             // var query = "SELECT sv.sampleid, sv.chromosome, sv.startposition, sv.endposition, sv.referenceallele, sv.alternateallele, sv.genotype0, sv.genotype1, count(*)/cast(numsamples AS DOUBLE) AS genotypefrequency, cv.rsid, cv.genesymbol, cv.clinicalsignificance, cv.phenotypelist FROM variants sv CROSS JOIN (SELECT count(1) AS numsamples FROM (SELECT DISTINCT sampleid FROM variants WHERE sampleid LIKE 'NA12%')) JOIN annotations cv ON sv.chromosome = cv.chromosome AND sv.startposition = cv.startposition - 1 AND sv.endposition = cv.endposition AND sv.referenceallele = cv.referenceallele AND sv.alternateallele = cv.alternateallele WHERE assembly='GRCh37' AND cv.clinicalsignificance LIKE '%Pathogenic%' AND sampleid LIKE 'NA12%' GROUP BY  sv.sampleid, sv.chromosome, sv.startposition, sv.endposition, sv.referenceallele, sv.alternateallele, sv.genotype0, sv.genotype1, cv.clinicalsignificance, cv.genesymbol, cv.phenotypelist, cv.rsid, numsamples ORDER BY  genotypefrequency DESC LIMIT 50"
            var params = {
              // QueryString: query,
              QueryString: event.query,
              ResultConfiguration: {
                OutputLocation: !Ref ResultsBucket,
                EncryptionConfiguration: {
                  EncryptionOption: 'SSE_S3'
                }
              },
              QueryExecutionContext: {
                Database: '${AWS::StackName}'
              }
            };
            var queryId;
            startQueryExecution(params)
            .then(function(data) {
              queryId = data.QueryExecutionId;
              return pollForChanges(queryId);
            })
            .then(function(state) {
              return getQueryResults(queryId);
            })
            .then(function(data){
              console.log('Results are ...');
              context.succeed(data);
            })
            .catch(function(err) {
              console.log(err, err.stack);
            });
          };
  AssetsBucket: # S3 Bucket for static web site
    Type: 'AWS::S3::Bucket'
    Properties:
      WebsiteConfiguration:
        IndexDocument: 'index.html'
  S3BucketPolicy: # Bucket Policy for static web site
      Type: 'AWS::S3::BucketPolicy'
      Properties:
        Bucket: !Ref AssetsBucket
        PolicyDocument:
          Statement:
          - Action:
            - 's3:GetObject'
            Effect: Allow
            Resource:
            - !Sub 'arn:aws:s3:::${AssetsBucket}/*'
            Principal: '*'
          - Action:
            - 's3:ListBucket'
            Effect: Allow
            Resource:
            - !GetAtt 'AssetsBucket.Arn'
            Principal: '*'
  AssetsCDN: # CloudFront configuration of static web site
    Type: 'AWS::CloudFront::Distribution'
    Properties:
      DistributionConfig:
        Origins:
          - DomainName: !GetAtt
              - AssetsBucket
              - DomainName
            Id: AssetsBucket
            S3OriginConfig: {}
        Enabled: true
        HttpVersion: http2
        # Aliases:
        #   - !Ref pDomainName
        DefaultCacheBehavior:
          ForwardedValues:
            QueryString: false
            Cookies:
              Forward: none
          TargetOriginId: AssetsBucket
          ViewerProtocolPolicy: !If [cHasForceHttps, 'redirect-to-https', 'allow-all']
        # ViewerCertificate:
        #   AcmCertificateArn: !Ref pCFCertificate
        #   SslSupportMethod: sni-only
        PriceClass: PriceClass_100
        CustomErrorResponses:
          - ErrorCode: 404
            ResponseCode: 200
            ResponsePagePath: '/index.html'
        DefaultRootObject: 'index.html'
  rVCFBucket: # Creates S3 Bucket to upload vcf file
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties:
      BucketName: !Ref pVCFBucket
      BucketEncryption: 
        ServerSideEncryptionConfiguration: 
        - ServerSideEncryptionByDefault:
            SSEAlgorithm: AES256
      Tags:
      - Key: Name
        Value: "VCF input S3 bucket"
      NotificationConfiguration:
        LambdaConfigurations:
          -
            Function: !GetAtt [rLambdaEMRAdamCluster, Arn]
            Event: "s3:ObjectCreated:*"
            Filter:
              S3Key:
                Rules:
                  -
                    Name: suffix
                    Value: .vcf.gz
  rBucketPermission: # Creates perssion to allow Lambda to access VCF S3 bucket
    Type: AWS::Lambda::Permission
    Properties:
      Action: 'lambda:InvokeFunction'
      FunctionName: !Ref rLambdaEMRAdamCluster
      Principal: s3.amazonaws.com
      SourceAccount: !Ref "AWS::AccountId"
      SourceArn: !Sub "arn:${AWS::Partition}:s3:::${pVCFBucket}"
  rNotificationBucketPolicy: # Creates policy to allow putBucket Notification
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref rVCFBucket
      PolicyDocument:
        Statement:
          - Effect: "Allow"
            Action:
            - 's3:PutBucketNotification'
            Resource: !Sub "arn:${AWS::Partition}:s3:::${pVCFBucket}"
            Principal:
              AWS: !GetAtt [rLambdaRole, Arn]
  rVariantsBucket: # Creates S3 Bucket to write output Parquet files
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    Properties:
      BucketName: !Ref pVariantsBucket
      BucketEncryption: 
        ServerSideEncryptionConfiguration: 
        - ServerSideEncryptionByDefault:
            SSEAlgorithm: AES256
      Tags:
      - Key: Name
        Value: "Parquet output S3 bucket"
  rInstanceProfile: # Creates EMR Instance Profile
    Properties:
      Path: "/"
      Roles:
      - Ref: rEMREC2Role
    Type: AWS::IAM::InstanceProfile
  rEMREC2Role: # Creates IAM role for EMR to access EC2
    Type: AWS::IAM::Role
    Properties:
      Path: "/"
      AssumeRolePolicyDocument:
        Statement:
        - Action:
          - sts:AssumeRole
          Effect: Allow
          Principal:
            Service:
            - ec2.amazonaws.com
        Version: '2012-10-17'
      ManagedPolicyArns:
      - !Sub arn:${AWS::Partition}:iam::aws:policy/service-role/AmazonElasticMapReduceforEC2Role
      Policies:
      - PolicyName: Athena-for-EMR
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Resource: "*"
            Action:
            - athena:*
            Effect: Allow
  rEMRServiceRole: # Creates IAM role for EMR Service access
    Type: AWS::IAM::Role
    Properties:
      Path: "/"
      AssumeRolePolicyDocument:
        Statement:
        - Action:
          - sts:AssumeRole
          Effect: Allow
          Principal:
            Service:
            - elasticmapreduce.amazonaws.com
        Version: '2012-10-17'
      ManagedPolicyArns:
      - !Sub arn:${AWS::Partition}:iam::aws:policy/service-role/AmazonElasticMapReduceRole
  rEMRAutoScalingDefaultRole: # Creates EMR Autoscaling IAM Role
    Type: AWS::IAM::Role
    Properties:
      Path: "/"
      AssumeRolePolicyDocument:
        Statement:
        - Action:
          - sts:AssumeRole
          Effect: Allow
          Principal:
            Service:
            - elasticmapreduce.amazonaws.com
            - application-autoscaling.amazonaws.com
        Version: '2012-10-17'
      ManagedPolicyArns:
      - !Sub arn:${AWS::Partition}:iam::aws:policy/service-role/AmazonElasticMapReduceforAutoScalingRole # EMR AutoScalingRole # EMR Autoscaling Default Role # Creates EMR Autoscaling Default Role
  rLambdaRole: # Role resource that the Lambda function uses.
      Type: AWS::IAM::Role
      Properties:
        AssumeRolePolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Sid: ''
            Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
        # Give Lambda function role name unique pertaining to stack name to avoid IAM role naming conflicts
        RoleName: !Sub
          - LambdaRoleFor-${StackName}
          - { StackName: !Ref 'AWS::StackName' }
        Policies:
        - PolicyName: emr-lambda-role
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
            - Effect: Allow
              Action:
              - elasticmapreduce:RunJobFlow
              - elasticmapreduce:PutAutoScalingPolicy
              - elasticmapreduce:AddInstanceGroups
              - elasticmapreduce:AddJobFlowSteps
              - elasticmapreduce:CreateSecurityConfiguration
              - elasticmapreduce:SetVisibleToAllUsers
              - elasticmapreduce:ListClusters
              - elasticmapreduce:DescribeCluster
              - elasticmapreduce:DescribeStep
              Resource: '*'
            - Effect: Allow
              Action:
              - iam:CreateServiceLinkedRole
              - iam:GetRole
              - iam:PassRole
              Resource: '*'
            - Effect: Allow
              Action:
              - s3:Get*
              - s3:List*
              - s3:Put*
              - cloudwatch:*
              - logs:*
              - sns:publish
              Resource: '*'
  rLambdaEMRAdamCluster: # Lambda function code resource
    Type: AWS::Lambda::Function
    Properties:
      Tags:
      - Key: Name
        Value: "EMR-ADAM-LambdaFunction"
      Environment:
        Variables:
          emr_release_label: !Ref 'pEMRRelease'
          master_instance_type: !Ref 'pMasterInstanceType'
          core_instance_type: !Ref 'pCoreInstanceType'
          no_core_instances: !Ref 'pNumberofNodes'
          subnet_id: !Ref 'pSubnet'
          ec2_keypair: !Ref 'pEC2KeyName'
          vcf_location: !Ref 'pVCFBucket'
          code_location: !Ref 'pEmrCodeBucket'
          parquet_location: !Ref 'pVariantsBucket'
          outbound_topic_arn: !Ref 'rSNSCreateSNSTopic'
          driver_memory:  !Ref 'pDriverMemory'
          executor_memory:  !Ref 'pExecutorMemory'
          logging_level: !Ref 'pLoggingLevel'
          ec2_role: !Ref 'rInstanceProfile'
          emr_service_role: !Ref 'rEMRServiceRole'
      Handler: index.lambda_handler
      FunctionName: !Sub
          - EMR-${StackName}
          - { StackName: !Ref 'AWS::StackName' }
      Description: Lambda function to create new EMR Cluster and Bootstrap scripts.
      Role: !GetAtt [rLambdaRole, Arn]
      Code:
        S3Bucket: !Ref 'pEmrCodeBucket'
        S3Key: lambda.zip
      Runtime: python3.6
      Timeout: 180
  rCloudWatchEvent: #CloudWatch Event Rule to trigger the Lambda Function
    Type: AWS::Events::Rule
    Properties:
      EventPattern:
          source:
            - aws.emr
          detail-type:
              - "EMR Step Status Change"
          detail:
            state:
              - COMPLETED
              - FAILED
      Targets:
      - Arn:
          Fn::GetAtt:
            - "rLambdaEMRAdamCluster"
            - "Arn"
        Id: rLambdaEMRAdamCluster
  rCloudWatchEventLambdaPermission: # Allows CloudWatch Event to Invoke the rLambdaFunction function
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName:
        Fn::GetAtt:
          - "rLambdaEMRAdamCluster"
          - "Arn"
      Principal: events.amazonaws.com
      SourceArn: !GetAtt rCloudWatchEvent.Arn
  rSNSCreateSNSTopic: #SNS Topic Creation and Subscription of a single Email address
    Type: AWS::SNS::Topic
    Properties:
      Subscription:
      - Endpoint: !Ref pSNSEmailAddress
        Protocol: email
  rGlueDatabase: # Glue Database
      Type: AWS::Glue::Database
      Properties:
        CatalogId: !Ref AWS::AccountId
        DatabaseInput:
          Name: !Ref pGlueDatabase
          Description: "vqt-db"
  rGlueVariantsTable: # Glue Variants Table
    Type: AWS::Glue::Table
    Properties:
      DatabaseName: !Ref rGlueDatabase
      CatalogId: !Ref AWS::AccountId
      TableInput:
        Name: variants
        Parameters: { "classification" : "parquet" }
        StorageDescriptor:
          Location:
            Fn::Sub: "s3://${rVariantsBucket}/clean"
          InputFormat: "org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat"
          OutputFormat: "org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat"
          SerdeInfo:
            SerializationLibrary: "org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe"
          Columns:
          - Name: variant
            Type: struct<contigName:string,start:bigint,end:bigint,names:array<string>,splitFromMultiAllelic:boolean,referenceAllele:string,alternateAllele:string,quality:double,filtersApplied:boolean,filtersPassed:boolean,filtersFailed:array<string>,annotation:struct<ancestralAllele:string,alleleCount:int,readDepth:int,forwardReadDepth:int,reverseReadDepth:int,referenceReadDepth:int,referenceForwardReadDepth:int,referenceReverseReadDepth:int,alleleFrequency:float,cigar:string,dbSnp:boolean,hapMap2:boolean,hapMap3:boolean,validated:boolean,thousandGenomes:boolean,somatic:boolean,transcriptEffects:array<struct<alternateAllele:string,effects:array<string>,geneName:string,geneId:string,featureType:string,featureId:string,biotype:string,rank:int,total:int,genomicHgvs:string,transcriptHgvs:string,proteinHgvs:string,cdnaPosition:int,cdnaLength:int,cdsPosition:int,cdsLength:int,proteinPosition:int,proteinLength:int,distance:int,messages:array<string>>>,attributes:map<string,string>>>
          - Name: contigname
            Type: string
          - Name: start
            Type: bigint
          - Name: end
            Type: bigint
          - Name: variantcallingannotations
            Type: struct<filtersApplied:boolean,filtersPassed:boolean,filtersFailed:array<string>,downsampled:boolean,baseQRankSum:float,fisherStrandBiasPValue:float,rmsMapQ:float,mapq0Reads:int,mqRankSum:float,readPositionRankSum:float,genotypePriors:array<float>,genotypePosteriors:array<float>,vqslod:float,culprit:string,attributes:map<string,string>>
          - Name: sampleid
            Type: string
          - Name: sampledescription
            Type: string
          - Name: processingdescription
            Type: string
          - Name: alleles
            Type: array<string>
          - Name: expectedalleledosage
            Type: float
          - Name: referencereaddepth
            Type: int
          - Name: alternatereaddepth
            Type: int
          - Name: readdepth
            Type: int
          - Name: minreaddepth
            Type: int
          - Name: genotypequality
            Type: int
          - Name: genotypelikelihoods
            Type: array<double>
          - Name: nonreferencelikelihoods
            Type: array<double>
          - Name: strandbiascomponents
            Type: array<int>
          - Name: splitfrommultiallelic
            Type: boolean
          - Name: phased
            Type: boolean
          - Name: phasesetid
            Type: int
          - Name: phasequality
            Type: int
  rGlueAnnotationsTable: # Glue Annotations Table
    Type: AWS::Glue::Table
    Properties:
      DatabaseName: !Ref rGlueDatabase
      CatalogId: !Ref AWS::AccountId
      TableInput:
        Name: annotations
        Parameters: { "classification" : "parquet" }
        StorageDescriptor:
          Location:
            Fn::Sub: "s3://${pAnnotationsBucket}/annotations"
          InputFormat: "org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat"
          OutputFormat: "org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat"
          SerdeInfo:
            SerializationLibrary: "org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe"
          Columns:
          - Name: contigname
            Type: string
          - Name: start
            Type: bigint
          - Name: end
            Type: bigint
          - Name: names
            Type: array<string>
          - Name: splitfrommultiallelic
            Type: boolean
          - Name: referenceallele
            Type: string
          - Name: alternateallele
            Type: string
          - Name: quality
            Type: double
          - Name: filtersapplied
            Type: boolean
          - Name: filterspassed
            Type: boolean
          - Name: filtersfailed
            Type: array<string>
          - Name: annotation
            Type: struct<ancestralAllele:string,alleleCount:int,readDepth:int,forwardReadDepth:int,reverseReadDepth:int,referenceReadDepth:int,referenceForwardReadDepth:int,referenceReverseReadDepth:int,alleleFrequency:float,cigar:string,dbSnp:boolean,hapMap2:boolean,hapMap3:boolean,validated:boolean,thousandGenomes:boolean,somatic:boolean,transcriptEffects:array<struct<alternateAllele:string,effects:array<string>,geneName:string,geneId:string,featureType:string,featureId:string,biotype:string,rank:int,total:int,genomicHgvs:string,transcriptHgvs:string,proteinHgvs:string,cdnaPosition:int,cdnaLength:int,cdsPosition:int,cdsLength:int,proteinPosition:int,proteinLength:int,distance:int,messages:array<string>>>,attributes:map<string,string>>
Outputs:
  AssetsBucket:
    Value: !Ref AssetsBucket
  CloudFrontDistribution:
    Value: !Ref AssetsCDN
  CloudFrontDomainName:
    Value: !GetAtt 'AssetsCDN.DomainName'
  IdentityPoolId:
    Value: !Ref IdentityPool
  oLambdaFunctionforEMRAdam: # Lambda Function created to launch ADAM- EMR cluster
    Description: Lambda function to launch EMR cluster_name
    Value: !Ref rLambdaEMRAdamCluster
  oVCFBucket: # VCF Input S3 bucket
    Description: VCF file input S3 bucket
    Value: !Ref rVCFBucket
  oVariantsBucket: # Parquet Output S3 bucket
    Description: Parquet file output S3 bucket
    Value: !Ref rVariantsBucket
  oSNSSubscriber: # EMAIL ID subscribed for notification
    Description: SNS Subscriber Email Address
    Value: !Ref pSNSEmailAddress
    Export:
      Name: !Sub ${AWS::StackName}-NotificationEmail
  oSNSTopicARN: # SNS topic
    Description: SNS topic ARN
    Value: !Ref rSNSCreateSNSTopic
    Export:
      Name: !Sub ${AWS::StackName}-SNSARN
  oGlueDatabase: # Name of Athena Database created
    Description: Athena Database for Parquet S3 files
    Value: !Ref rGlueDatabase
  oGlueVariantsTable: # Name of Athena Variants Table created
    Description: Athena Table for Variants from Parquet S3 files
    Value: !Ref rGlueVariantsTable
